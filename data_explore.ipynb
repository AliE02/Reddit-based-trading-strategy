{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Extracting relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of keys that we will consider in our sentiment analysis task are the following:\n",
    "- `selftext`: the text of the post, will contain nuances of the sentiment\n",
    "- `title`: the title of the post often refers to the asset or the topic of the post\n",
    "- `created` or `created_utc`: is the date of creation of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed submissions that had `[deleted]` or `[removed]` in either their text or title. <br>\n",
    "This means that the post was moderated and we cannot use it for our analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json file with data that only contains the above keys\n",
    "data = []\n",
    "with open(\"Data/wallstreetbets_submissions\", \"r\") as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        # check if the title or text contains the string \"[removed]\"\n",
    "        if \"[removed]\" not in d['title'] and \"[removed]\" not in d['selftext'] and \"[deleted]\" not in d['title'] and \"[deleted]\" not in d['selftext']:\n",
    "            data.append(\n",
    "                {\n",
    "                    'date': d['created_utc'] if 'created_utc' in d else d['created'],\n",
    "                    'title': d['title'],\n",
    "                    'text': d['selftext'],\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **822004** such submissions that we will further preprocess in order to perform sentiment analysis.<br>\n",
    "These submissions are going to aliment our sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data file can be found under `Data/wallstreetbets_submissions_cleaned.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to a new file in a well formatted way\n",
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SPY Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to check how many submissions are related to the S&P 500 ETF (SPY).<br>\n",
    "For this we will be looking for the keywords:\n",
    "- `SPY`\n",
    "- `SP500`\n",
    "- `S&P500`\n",
    "- `S&P 500`\n",
    "- `S&P`\n",
    "- `Standard & Poor's`\n",
    "- `Standard and Poor's`\n",
    "- `Standard and Poor 500`\n",
    "- `Standard & Poor 500`\n",
    "- `Standard and Poor 500`\n",
    "- `Standard & Poor`\n",
    "- `Standard and Poor`\n",
    "- `Standard and Poor's 500`\n",
    "- `Standard & Poor's 500`\n",
    "- `Standard and Poor's 500`\n",
    "in the title and the text <br>\n",
    "We will be using regexes to find these keywords in an optimized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of regexes that match the words in the above list\n",
    "words = [\"spy\", \"sp500\", \"s&p\", \"standard & poor\", \"standard and poor\"]\n",
    "\n",
    "# create a regex pattern that matches any of the words in the above list\n",
    "pattern = regex.compile(r'\\b(?:' + '|'.join(words) + r')\\b', regex.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data = []\n",
    "\n",
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for d in data:\n",
    "        if pattern.search(d['title']) or pattern.search(d['text']):\n",
    "            spy_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data related to SPY can be found under `Data/wallstreetbets_submissions_SPY.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/wallstreetbets_submissions_spy.json\", \"w\") as f:\n",
    "    json.dump(spy_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea on the period that is spanned by the data related to SPY, we will plot the number of submissions per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_count = {}\n",
    "utc_zone = timezone.utc\n",
    "\n",
    "for d in spy_data:\n",
    "    date = datetime.fromtimestamp(int(d['date']), tz=utc_zone)\n",
    "    if date in date_count:\n",
    "        date_count[date] += 1\n",
    "    else:\n",
    "        date_count[date] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Finding the right assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check the data for keywords that are related to specific stocks. <br>\n",
    "We will be looking for the keywords:\n",
    "- `AAPL`\n",
    "- `TSLA`\n",
    "- `NVDA`\n",
    "- `AMZN`\n",
    "- `LVMH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn = [\n",
    "    \"amazon\",\n",
    "    \"primeday\", \n",
    "    \"alexa\", \n",
    "    \"kindle\", \n",
    "    \"aws\",\n",
    "    \"amzn\",\n",
    "    \"prime\",\n",
    "    \"bezos\",\n",
    "    \"e-commerce giant\"\n",
    "]\n",
    "\n",
    "tsla = [\n",
    "    \"tsla\",\n",
    "    \"tesla\",\n",
    "    \"elon\",\n",
    "    \"musk\",\n",
    "    \"tesla\",\n",
    "    \"model s\",\n",
    "    \"model x\",\n",
    "    \"model 3\",\n",
    "    \"model y\",\n",
    "    \"gigafactory\",\n",
    "    \"electric vehicles\",\n",
    "    \"ev\",\n",
    "    \"autopilot\"\n",
    "]\n",
    "\n",
    "nvda = [\n",
    "    \"nvda\",\n",
    "    \"nvidia\",\n",
    "    \"jensen huang\",  # CEO of nvidia\n",
    "    \"geforce\",  # nvidia's popular gPU brand\n",
    "    \"turing\",  # nvidia's gPU architecture\n",
    "    \"ampere\",  # A more recent nvidia gPU architecture\n",
    "    \"rtx\",  # Ray tracing gPUs\n",
    "    \"gtx\",  # Previous generation gPUs\n",
    "    \"quadro\",  # Professional graphics cards\n",
    "    \"tesla gpu\",  # not to be confused with Tesla Motors\n",
    "    \"cuda\",  # nvidia's parallel computing platform\n",
    "    \"dlss\",  # Deep Learning Super Sampling, nvidia's AI-driven image upscaling technology\n",
    "    \"ray tracing\",  # graphics rendering technique that nvidia gPUs support\n",
    "    \"g-sync\",  # nvidia's display technology\n",
    "    \"omniverse\"  # nvidia's collaboration and simulation platform\n",
    "]\n",
    "\n",
    "aapl = [\n",
    "    \"aapl\",\n",
    "    \"apple\",\n",
    "    \"tim cook\"\n",
    "    \"steve jobs\"\n",
    "    \"iphone\",\n",
    "    \"ipad\",\n",
    "    \"mac\",\n",
    "    \"ios\",\n",
    "    \"app store\",\n",
    "    \"icloud\",\n",
    "    \"siri\",\n",
    "    \"wwdc\",  # Apple's Worldwide Developers Conference\n",
    "    \"m1\",\n",
    "    \"m2\",\n",
    "    \"face id\",\n",
    "    \"touch id\",\n",
    "    \"homepod\",\n",
    "    \"airtag\",\n",
    "    \"Aarkit\",  # Apple's AR platform\n",
    "    \"carplay\",\n",
    "    \"airpods\"\n",
    "]\n",
    "\n",
    "lvmh = [\n",
    "    \"lvmh\",\n",
    "    \"bernard arnault\",\n",
    "    \"louis vuitton\",\n",
    "    \"dior\",\n",
    "    \"givenchy\",\n",
    "    \"Fendi\",\n",
    "    \"céline\",\n",
    "    \"kenzo\",\n",
    "    \"marc jacobs\",\n",
    "    \"Hublot\",\n",
    "    \"tag Heuer\",\n",
    "    \"sephora\",\n",
    "    \"moët & chandon\",\n",
    "    \"dom pérignon\",\n",
    "    \"hennessy\",\n",
    "    \"cognac\",\n",
    "    \"loro piana\",\n",
    "    \"dfs group\",\n",
    "    \"la samaritaine\",\n",
    "    \"cheval blanc\",\n",
    "]\n",
    "\n",
    "wmt = [\n",
    "    \"wmt\",\n",
    "    \"walmart\",\n",
    "    \"doug mcmillon\",\n",
    "    \"sam walton\",\n",
    "    \"walton family\",\n",
    "    \"supercenter\",\n",
    "    \"hypermarket\",\n",
    "    \"neighborhood market\",\n",
    "    \"sam's\"\n",
    "]\n",
    "\n",
    "goog = [\n",
    "    \"goog\",\n",
    "    \"google\",\n",
    "    \"alphabet\",\n",
    "    \"larry page\",\n",
    "    \"sergey brin\",\n",
    "    \"sundar pichai\",\n",
    "    \"android\",\n",
    "    \"chrome\",\n",
    "    \"search\",\n",
    "    \"maps\",\n",
    "    \"youtube\",\n",
    "    \"play store\",\n",
    "    \"pixel\",\n",
    "    \"nexus\",\n",
    "    \"nest\",\n",
    "    \"waymo\",\n",
    "    \"deepmind\",\n",
    "    \"loons\",\n",
    "    \"sidewalk labs\",\n",
    "    \"fitbit\",\n",
    "    \"waze\",\n",
    "    \"doubleclick\",\n",
    "    \"admob\",\n",
    "    \"adsense\",\n",
    "    \"adwords\",\n",
    "    \"gmail\",\n",
    "    \"drive\",\n",
    "    \"photos\",\n",
    "    \"podcasts\",\n",
    "    \"books\",\n",
    "    \"stadia\",\n",
    "    \"pay\",\n",
    "    \"wallet\",\n",
    "    \"shopping\",\n",
    "    \"one\",\n",
    "    \"workspace\",\n",
    "    \"cloud\",\n",
    "    \"gsuite\",\n",
    "    \"firebase\",\n",
    "    \"colab\",\n",
    "    \"bigquery\",\n",
    "    \"kubernetes\",\n",
    "    \"chromeos\",\n",
    "    \"chromebook\",\n",
    "    \"chromecast\",\n",
    "    \"nexus\",\n",
    "    \"pixel\",\n",
    "    \"pixelbook\",\n",
    "    \"pixelbuds\",\n",
    "    \"voice\",\n",
    "    \"hangouts\",\n",
    "    \"duo\",\n",
    "    \"meet\",\n",
    "    \"gmail\",\n",
    "    \"maps\",\n",
    "    \"street view\"\n",
    "]\n",
    "\n",
    "spy = [\"spy\", \"sp500\", \"s&p\", \"standard & poor\", \"standard and poor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_zone = timezone.utc\n",
    "\n",
    "def check_keywords(comments, keywords):\n",
    "    \"\"\"\n",
    "    This function checks if any of the keywords are present in the title or text of the submission.\n",
    "    It returns a dict of {date: count} where count is the number of keywords found in the submission.\n",
    "    \"\"\"\n",
    "    # create a regex pattern that matches any of the words in the above list\n",
    "    pattern = regex.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', regex.IGNORECASE)\n",
    "\n",
    "    keyword_count = {}\n",
    "    for c in comments:\n",
    "        # look for the keywords in the title and text of the submission in lower case using the regex library\n",
    "        if pattern.search(c['text'].lower()) or pattern.search(c['title'].lower()):\n",
    "            date = datetime.fromtimestamp(int(c['date']), tz=utc_zone).date()\n",
    "            if date in keyword_count:\n",
    "                keyword_count[date] += 1\n",
    "            else:\n",
    "                keyword_count[date] = 1\n",
    "\n",
    "    return keyword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import numpy as np  # Needed for mathematical operations like mean and median\n",
    "\n",
    "def plot_date_counts(title, date_counts):\n",
    "    # Sort the dictionary by date to ensure the plot is ordered\n",
    "    sorted_dates = sorted(date_counts.items())\n",
    "    dates, counts = zip(*sorted_dates)\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_count = np.mean(counts)\n",
    "    median_count = np.median(counts)\n",
    "    max_count = max(counts)\n",
    "    min_count = min(counts)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(dates, counts, marker='o', linestyle='-', color='b', markersize=2, linewidth=1)\n",
    "\n",
    "    # Add horizontal lines for mean and median\n",
    "    plt.axhline(y=mean_count, color='r', linestyle='--', linewidth=1, label=f'Mean: {mean_count:.2f}')\n",
    "    plt.axhline(y=median_count, color='g', linestyle=':', linewidth=1, label=f'Median: {median_count:.2f}')\n",
    "\n",
    "    # Formatting the date display on the x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=7))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.gcf().autofmt_xdate()  # Rotation\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend()  # Add legend to the plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display statistics in the plot\n",
    "    plt.figtext(0.99, 0.01, f'Max: {max_count}\\nMin: {min_count}', horizontalalignment='right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"r\") as f:\n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking TSLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_dict = check_keywords(comments, tsla)\n",
    "nvda_dict = check_keywords(comments, nvda)\n",
    "amzn_dict = check_keywords(comments, amzn)\n",
    "aapl_dict = check_keywords(comments, aapl)\n",
    "lvmh_dict = check_keywords(comments, lvmh)\n",
    "wmt_dict = check_keywords(comments, wmt)\n",
    "goog_dict = check_keywords(comments, goog)\n",
    "spy_dict = check_keywords(comments, spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date of the maximum count for each stock\n",
    "print(\"tsla max date: \", max(tsla_dict, key=tsla_dict.get))\n",
    "print(\"nvda max date: \", max(nvda_dict, key=nvda_dict.get))\n",
    "print(\"amzn max date: \", max(amzn_dict, key=amzn_dict.get))\n",
    "print(\"aapl max date: \", max(aapl_dict, key=aapl_dict.get))\n",
    "print(\"lvmh max date: \", max(lvmh_dict, key=lvmh_dict.get))\n",
    "print(\"wmt max date: \", max(wmt_dict, key=wmt_dict.get))\n",
    "print(\"goog max date: \", max(goog_dict, key=goog_dict.get))\n",
    "print(\"spy max date: \", max(spy_dict, key=spy_dict.get))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"TSLA mentions\", tsla_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"NVDA mentions\", nvda_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"AMZN mentions\", amzn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"AAPL mentions\", aapl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"LVMH mentions\", lvmh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"WMT mentions\", wmt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"GOOG mentions\", goog_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"SPY mentions\", spy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/tweets_labelled_09042020_16072020.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count non nan sentiment values\n",
    "sentiment_count = df['sentiment'].value_counts()\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/stock_data.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = df2['Sentiment'].value_counts()\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Creating the Dataset of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spy data\n",
    "with open(\"Data/wsb_spy.json\", \"r\") as f:\n",
    "    spy_data = json.load(f)\n",
    "\n",
    "# create a dataframe from the spy data\n",
    "spy_df = pd.DataFrame(spy_data)\n",
    "\n",
    "spy_prices = pd.read_csv(\"Data/spy_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/sr110gkn1vz4qf70llfwlh8c0000gn/T/ipykernel_3334/8274418.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  spy_df['date'] = pd.to_datetime(spy_df['date'], unit='s').dt.to_period('M')\n"
     ]
    }
   ],
   "source": [
    "# convert the date column to datetime in month format\n",
    "spy_df['date'] = pd.to_datetime(spy_df['date'], unit='s').dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by date and take the average sentiment and the count of submissions\n",
    "spy_grouped = spy_df.groupby('date').agg({'prediction': 'mean', 'text': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column Date of the spy_prices dataframe to datetime format period month\n",
    "spy_prices['Date'] = pd.to_datetime(spy_prices['Date']).dt.to_period('M')\n",
    "\n",
    "# change the name of the column Date to date\n",
    "spy_prices = spy_prices.rename(columns={'Date': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes on the date column and only keep the column Close from the spy_prices dataframe\n",
    "spy_grouped = spy_grouped.merge(spy_prices[['date', 'Close']], on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns to something more meaningful\n",
    "spy_grouped = spy_grouped.rename(columns={'prediction': 'average_sentiment', 'text': 'submission_count', 'Close': 'close_price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write spy_grouped to a csv file\n",
    "spy_grouped.to_csv(\"Data/spy_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
