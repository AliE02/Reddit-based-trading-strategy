{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import regex\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Extracting relevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of keys that we will consider in our sentiment analysis task are the following:\n",
    "- `selftext`: the text of the post, will contain nuances of the sentiment\n",
    "- `title`: the title of the post often refers to the asset or the topic of the post\n",
    "- `created` or `created_utc`: is the date of creation of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We removed submissions that had `[deleted]` or `[removed]` in either their text or title. <br>\n",
    "This means that the post was moderated and we cannot use it for our analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new json file with data that only contains the above keys\n",
    "data = []\n",
    "with open(\"Data/raw/stocks_submissions\", \"r\") as f:\n",
    "    for line in f:\n",
    "        d = json.loads(line)\n",
    "        # check if the title or text contains the string \"[removed]\"\n",
    "        if \"[removed]\" not in d['title'] and \"[removed]\" not in d['selftext'] and \"[deleted]\" not in d['title'] and \"[deleted]\" not in d['selftext']:\n",
    "            data.append(\n",
    "                {\n",
    "                    'date': d['created_utc'] if 'created_utc' in d else d['created'],\n",
    "                    'title': d['title'],\n",
    "                    'text': d['selftext'],\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to a new file in a well formatted way\n",
    "with open(\"Data/stocks.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **822004** such submissions that we will further preprocess in order to perform sentiment analysis.<br>\n",
    "These submissions are going to aliment our sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned data file can be found under `Data/wallstreetbets_submissions_cleaned.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to a new file in a well formatted way\n",
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- SPY Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to check how many submissions are related to the S&P 500 ETF (SPY).<br>\n",
    "For this we will be looking for the keywords:\n",
    "- `SPY`\n",
    "- `SP500`\n",
    "- `S&P500`\n",
    "- `S&P 500`\n",
    "- `S&P`\n",
    "- `Standard & Poor's`\n",
    "- `Standard and Poor's`\n",
    "- `Standard and Poor 500`\n",
    "- `Standard & Poor 500`\n",
    "- `Standard and Poor 500`\n",
    "- `Standard & Poor`\n",
    "- `Standard and Poor`\n",
    "- `Standard and Poor's 500`\n",
    "- `Standard & Poor's 500`\n",
    "- `Standard and Poor's 500`\n",
    "in the title and the text <br>\n",
    "We will be using regexes to find these keywords in an optimized way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of regexes that match the words in the above list\n",
    "words = [\"spy\", \"sp500\", \"s&p\", \"standard & poor\", \"standard and poor\"]\n",
    "\n",
    "# create a regex pattern that matches any of the words in the above list\n",
    "pattern = regex.compile(r'\\b(?:' + '|'.join(words) + r')\\b', regex.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_data = []\n",
    "\n",
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for d in data:\n",
    "        if pattern.search(d['title']) or pattern.search(d['text']):\n",
    "            spy_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data related to SPY can be found under `Data/wallstreetbets_submissions_SPY.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/wallstreetbets_submissions_spy.json\", \"w\") as f:\n",
    "    json.dump(spy_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have an idea on the period that is spanned by the data related to SPY, we will plot the number of submissions per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_count = {}\n",
    "utc_zone = timezone.utc\n",
    "\n",
    "for d in spy_data:\n",
    "    date = datetime.fromtimestamp(int(d['date']), tz=utc_zone)\n",
    "    if date in date_count:\n",
    "        date_count[date] += 1\n",
    "    else:\n",
    "        date_count[date] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Finding the right assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check the data for keywords that are related to specific stocks. <br>\n",
    "We will be looking for the keywords:\n",
    "- `AAPL`\n",
    "- `TSLA`\n",
    "- `NVDA`\n",
    "- `AMZN`\n",
    "- `LVMH`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn = [\n",
    "    \"amazon\",\n",
    "    \"primeday\", \n",
    "    \"alexa\", \n",
    "    \"kindle\", \n",
    "    \"aws\",\n",
    "    \"amzn\",\n",
    "    \"prime\",\n",
    "    \"bezos\",\n",
    "    \"e-commerce giant\"\n",
    "]\n",
    "\n",
    "tsla = [\n",
    "    \"tsla\",\n",
    "    \"tesla\",\n",
    "    \"elon\",\n",
    "    \"musk\",\n",
    "    \"tesla\",\n",
    "    \"model s\",\n",
    "    \"model x\",\n",
    "    \"model 3\",\n",
    "    \"model y\",\n",
    "    \"gigafactory\",\n",
    "    \"electric vehicles\",\n",
    "    \"ev\",\n",
    "    \"autopilot\"\n",
    "]\n",
    "\n",
    "nvda = [\n",
    "    \"nvda\",\n",
    "    \"nvidia\",\n",
    "    \"jensen huang\",  # CEO of nvidia\n",
    "    \"geforce\",  # nvidia's popular gPU brand\n",
    "    \"turing\",  # nvidia's gPU architecture\n",
    "    \"ampere\",  # A more recent nvidia gPU architecture\n",
    "    \"rtx\",  # Ray tracing gPUs\n",
    "    \"gtx\",  # Previous generation gPUs\n",
    "    \"quadro\",  # Professional graphics cards\n",
    "    \"tesla gpu\",  # not to be confused with Tesla Motors\n",
    "    \"cuda\",  # nvidia's parallel computing platform\n",
    "    \"dlss\",  # Deep Learning Super Sampling, nvidia's AI-driven image upscaling technology\n",
    "    \"ray tracing\",  # graphics rendering technique that nvidia gPUs support\n",
    "    \"g-sync\",  # nvidia's display technology\n",
    "    \"omniverse\"  # nvidia's collaboration and simulation platform\n",
    "]\n",
    "\n",
    "aapl = [\n",
    "    \"aapl\",\n",
    "    \"apple\",\n",
    "    \"tim cook\"\n",
    "    \"steve jobs\"\n",
    "    \"iphone\",\n",
    "    \"ipad\",\n",
    "    \"mac\",\n",
    "    \"ios\",\n",
    "    \"app store\",\n",
    "    \"icloud\",\n",
    "    \"siri\",\n",
    "    \"wwdc\",  # Apple's Worldwide Developers Conference\n",
    "    \"m1\",\n",
    "    \"m2\",\n",
    "    \"face id\",\n",
    "    \"touch id\",\n",
    "    \"homepod\",\n",
    "    \"airtag\",\n",
    "    \"Aarkit\",  # Apple's AR platform\n",
    "    \"carplay\",\n",
    "    \"airpods\"\n",
    "]\n",
    "\n",
    "lvmh = [\n",
    "    \"lvmh\",\n",
    "    \"bernard arnault\",\n",
    "    \"louis vuitton\",\n",
    "    \"dior\",\n",
    "    \"givenchy\",\n",
    "    \"Fendi\",\n",
    "    \"céline\",\n",
    "    \"kenzo\",\n",
    "    \"marc jacobs\",\n",
    "    \"Hublot\",\n",
    "    \"tag Heuer\",\n",
    "    \"sephora\",\n",
    "    \"moët & chandon\",\n",
    "    \"dom pérignon\",\n",
    "    \"hennessy\",\n",
    "    \"cognac\",\n",
    "    \"loro piana\",\n",
    "    \"dfs group\",\n",
    "    \"la samaritaine\",\n",
    "    \"cheval blanc\",\n",
    "]\n",
    "\n",
    "wmt = [\n",
    "    \"wmt\",\n",
    "    \"walmart\",\n",
    "    \"doug mcmillon\",\n",
    "    \"sam walton\",\n",
    "    \"walton family\",\n",
    "    \"supercenter\",\n",
    "    \"hypermarket\",\n",
    "    \"neighborhood market\",\n",
    "    \"sam's\"\n",
    "]\n",
    "\n",
    "goog = [\n",
    "    \"goog\",\n",
    "    \"google\",\n",
    "    \"alphabet\",\n",
    "    \"larry page\",\n",
    "    \"sergey brin\",\n",
    "    \"sundar pichai\",\n",
    "    \"android\",\n",
    "    \"chrome\",\n",
    "    \"youtube\",\n",
    "    \"play store\",\n",
    "    \"pixel\",\n",
    "    \"nexus\",\n",
    "    \"deepmind\",\n",
    "    \"adsense\",\n",
    "    \"gmail\",\n",
    "    \"gsuite\",\n",
    "    \"firebase\",\n",
    "    \"chromebook\",\n",
    "    \"chromecast\",\n",
    "    \"gmail\",\n",
    "    \"street view\"\n",
    "]\n",
    "\n",
    "spy = [\"spy\", \"sp500\", \"s&p\", \"standard & poor\", \"standard and poor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(data, stock):\n",
    "    stock_data = []\n",
    "    pattern = regex.compile(r'\\b(?:' + '|'.join(stock) + r')\\b', regex.IGNORECASE)\n",
    "    for d in data:\n",
    "        if pattern.search(d['title']) or pattern.search(d['text']):\n",
    "            stock_data.append(d)\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for each stock\n",
    "goog_data = get_stock_data(data, goog)\n",
    "tsla_data = get_stock_data(data, tsla)\n",
    "aapl_data = get_stock_data(data, aapl)\n",
    "amzn_data = get_stock_data(data, amzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write each stock's data to a new file in a well formatted way\n",
    "with open(\"Data/wallstreetbets_submissions_goog.json\", \"w\") as f:\n",
    "    json.dump(goog_data, f, indent=4)\n",
    "\n",
    "with open(\"Data/wallstreetbets_submissions_tsla.json\", \"w\") as f:\n",
    "    json.dump(tsla_data, f, indent=4)\n",
    "\n",
    "with open(\"Data/wallstreetbets_submissions_aapl.json\", \"w\") as f:\n",
    "    json.dump(aapl_data, f, indent=4)\n",
    "    \n",
    "with open(\"Data/wallstreetbets_submissions_amzn.json\", \"w\") as f:\n",
    "    json.dump(amzn_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_zone = timezone.utc\n",
    "\n",
    "def check_keywords(comments, keywords):\n",
    "    \"\"\"\n",
    "    This function checks if any of the keywords are present in the title or text of the submission.\n",
    "    It returns a dict of {date: count} where count is the number of keywords found in the submission.\n",
    "    \"\"\"\n",
    "    # create a regex pattern that matches any of the words in the above list\n",
    "    pattern = regex.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', regex.IGNORECASE)\n",
    "\n",
    "    keyword_count = {}\n",
    "    for c in comments:\n",
    "        # look for the keywords in the title and text of the submission in lower case using the regex library\n",
    "        if pattern.search(c['text'].lower()) or pattern.search(c['title'].lower()):\n",
    "            date = datetime.fromtimestamp(int(c['date']), tz=utc_zone).date()\n",
    "            if date in keyword_count:\n",
    "                keyword_count[date] += 1\n",
    "            else:\n",
    "                keyword_count[date] = 1\n",
    "\n",
    "    return keyword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import numpy as np  # Needed for mathematical operations like mean and median\n",
    "\n",
    "def plot_date_counts(title, date_counts):\n",
    "    # Sort the dictionary by date to ensure the plot is ordered\n",
    "    sorted_dates = sorted(date_counts.items())\n",
    "    dates, counts = zip(*sorted_dates)\n",
    "\n",
    "    # Calculate statistics\n",
    "    mean_count = np.mean(counts)\n",
    "    median_count = np.median(counts)\n",
    "    max_count = max(counts)\n",
    "    min_count = min(counts)\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(dates, counts, marker='o', linestyle='-', color='b', markersize=2, linewidth=1)\n",
    "\n",
    "    # Add horizontal lines for mean and median\n",
    "    plt.axhline(y=mean_count, color='r', linestyle='--', linewidth=1, label=f'Mean: {mean_count:.2f}')\n",
    "    plt.axhline(y=median_count, color='g', linestyle=':', linewidth=1, label=f'Median: {median_count:.2f}')\n",
    "\n",
    "    # Formatting the date display on the x-axis\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator(minticks=3, maxticks=7))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "    plt.gcf().autofmt_xdate()  # Rotation\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.legend()  # Add legend to the plot\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display statistics in the plot\n",
    "    plt.figtext(0.99, 0.01, f'Max: {max_count}\\nMin: {min_count}', horizontalalignment='right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/wallstreetbets_submissions_cleaned.json\", \"r\") as f:\n",
    "    comments = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking TSLA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsla_dict = check_keywords(comments, tsla)\n",
    "nvda_dict = check_keywords(comments, nvda)\n",
    "amzn_dict = check_keywords(comments, amzn)\n",
    "aapl_dict = check_keywords(comments, aapl)\n",
    "lvmh_dict = check_keywords(comments, lvmh)\n",
    "wmt_dict = check_keywords(comments, wmt)\n",
    "goog_dict = check_keywords(comments, goog)\n",
    "spy_dict = check_keywords(comments, spy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the date of the maximum count for each stock\n",
    "print(\"tsla max date: \", max(tsla_dict, key=tsla_dict.get))\n",
    "print(\"nvda max date: \", max(nvda_dict, key=nvda_dict.get))\n",
    "print(\"amzn max date: \", max(amzn_dict, key=amzn_dict.get))\n",
    "print(\"aapl max date: \", max(aapl_dict, key=aapl_dict.get))\n",
    "print(\"lvmh max date: \", max(lvmh_dict, key=lvmh_dict.get))\n",
    "print(\"wmt max date: \", max(wmt_dict, key=wmt_dict.get))\n",
    "print(\"goog max date: \", max(goog_dict, key=goog_dict.get))\n",
    "print(\"spy max date: \", max(spy_dict, key=spy_dict.get))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"TSLA mentions\", tsla_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"NVDA mentions\", nvda_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"AMZN mentions\", amzn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"AAPL mentions\", aapl_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"LVMH mentions\", lvmh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"WMT mentions\", wmt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"GOOG mentions\", goog_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_counts(\"SPY mentions\", spy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/tweets_labelled_09042020_16072020.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count non nan sentiment values\n",
    "sentiment_count = df['sentiment'].value_counts()\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"Data/stock_data.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_count = df2['Sentiment'].value_counts()\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Creating the Dataset of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(sentiment_path, price_path, year):\n",
    "    with open(sentiment_path, \"r\") as f:\n",
    "        sentiment_data = json.load(f)\n",
    "    \n",
    "    stock_df = pd.DataFrame(sentiment_data)\n",
    "    price_df = pd.read_csv(price_path)\n",
    "\n",
    "    # convert the date column to datetime and only keep the date part\n",
    "    stock_df['date'] = pd.to_datetime(stock_df['date'], unit='s').dt.date\n",
    "    price_df['Date'] = pd.to_datetime(price_df['Date']).dt.date\n",
    "\n",
    "    # keep only rows where year is 2016 or later\n",
    "    price_df = price_df[price_df['Date'] >= datetime(year, 1, 1).date()]\n",
    "    stock_df = stock_df[stock_df['date'] >= datetime(year, 1, 1).date()]\n",
    "\n",
    "    # rename the date column to match the other dataframe\n",
    "    price_df.rename(columns={'Date': 'date'}, inplace=True)\n",
    "\n",
    "    # group by date column and take the mean prediction and count of submissions\n",
    "    stock_df = stock_df.groupby('date').agg({'prediction': 'mean', 'text': 'count'}).reset_index()\n",
    "\n",
    "    # rename the columns prediction and text to avg_sentiment and submission_count\n",
    "    stock_df.rename(columns={'prediction': 'avg_sentiment', 'text': 'submission_count'}, inplace=True)\n",
    "\n",
    "    # add dates where there are no submissions in between the min and max date\n",
    "    min_date = min(stock_df['date'])\n",
    "    max_date = max(stock_df['date'])\n",
    "    all_dates = pd.date_range(start=min_date, end=max_date).to_list()\n",
    "    all_dates = [d.date() for d in all_dates]\n",
    "    missing_dates = [d for d in all_dates if d not in stock_df['date'].to_list()]\n",
    "    # fill stock_df with missing dates and put 0 for the avg_sentiment and submission_count\n",
    "    missing_df = pd.DataFrame(missing_dates, columns=['date'])\n",
    "    missing_df['avg_sentiment'] = 0\n",
    "    missing_df['submission_count'] = 0\n",
    "    stock_df = pd.concat([stock_df, missing_df], ignore_index=True)\n",
    "\n",
    "    # sort the dataframe by date\n",
    "    stock_df.sort_values('date', inplace=True)\n",
    "    \n",
    "\n",
    "    # merge the two dataframes on the date column\n",
    "    stock_df = pd.merge(stock_df, price_df, on='date', how='inner')\n",
    "\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/sr110gkn1vz4qf70llfwlh8c0000gn/T/ipykernel_1538/1109646162.py:9: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  stock_df['date'] = pd.to_datetime(stock_df['date'], unit='s').dt.date\n",
      "/var/folders/zj/sr110gkn1vz4qf70llfwlh8c0000gn/T/ipykernel_1538/1109646162.py:9: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  stock_df['date'] = pd.to_datetime(stock_df['date'], unit='s').dt.date\n",
      "/var/folders/zj/sr110gkn1vz4qf70llfwlh8c0000gn/T/ipykernel_1538/1109646162.py:9: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  stock_df['date'] = pd.to_datetime(stock_df['date'], unit='s').dt.date\n",
      "/var/folders/zj/sr110gkn1vz4qf70llfwlh8c0000gn/T/ipykernel_1538/1109646162.py:9: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
      "  stock_df['date'] = pd.to_datetime(stock_df['date'], unit='s').dt.date\n"
     ]
    }
   ],
   "source": [
    "amzn_df = create_df(\"Data/sentiments/amazon_sentiment.json\", \"Data/prices/AMZN.csv\", 2011)\n",
    "aapl_df = create_df(\"Data/sentiments/apple_sentiment.json\", \"Data/prices/AAPL.csv\", 2009)\n",
    "tsla_df = create_df(\"Data/sentiments/tesla_sentiment.json\", \"Data/prices/TSLA.csv\", 2011)\n",
    "goog_df = create_df(\"Data/sentiments/google_sentiment.json\", \"Data/prices/GOOG.csv\", 2009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to a new file in a well formatted way and be careful with the encoding as well as the separators\n",
    "amzn_df.to_csv(\"Data/final/amzn_data.csv\", sep=\";\", index=False)\n",
    "aapl_df.to_csv(\"Data/final/aapl_data.csv\", sep=\";\", index=False)\n",
    "tsla_df.to_csv(\"Data/final/tsla_data.csv\", sep=\";\", index=False)\n",
    "goog_df.to_csv(\"Data/final/goog_data.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spy data\n",
    "with open(\"Data/wsb_spy.json\", \"r\") as f:\n",
    "    spy_data = json.load(f)\n",
    "\n",
    "# create a dataframe from the spy data\n",
    "spy_df = pd.DataFrame(spy_data)\n",
    "\n",
    "spy_prices = pd.read_csv(\"Data/spy_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the date column to datetime in month format\n",
    "spy_df['date'] = pd.to_datetime(spy_df['date'], unit='s').dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by date and take the average sentiment and the count of submissions\n",
    "spy_grouped = spy_df.groupby('date').agg({'prediction': 'mean', 'text': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the column Date of the spy_prices dataframe to datetime format period month\n",
    "spy_prices['Date'] = pd.to_datetime(spy_prices['Date']).dt.to_period('M')\n",
    "\n",
    "# change the name of the column Date to date\n",
    "spy_prices = spy_prices.rename(columns={'Date': 'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes on the date column and only keep the column Close from the spy_prices dataframe\n",
    "spy_grouped = spy_grouped.merge(spy_prices[['date', 'Close']], on='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns to something more meaningful\n",
    "spy_grouped = spy_grouped.rename(columns={'prediction': 'average_sentiment', 'text': 'submission_count', 'Close': 'close_price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write spy_grouped to a csv file\n",
    "spy_grouped.to_csv(\"Data/spy_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Correlating sentiment and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_df = pd.read_csv(\"Data/final/aapl_data.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a moving average of the avg_sentiment\n",
    "aapl_df['avg_sentiment_ma'] = aapl_df['avg_sentiment'].rolling(window=15).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the avg_sentiment and Close columns per day\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aapl_df['date'], aapl_df['avg_sentiment_ma'], marker='o', linestyle='-', color='b', markersize=2, linewidth=1, label='avg_sentiment')\n",
    "plt.title(\"AAPL avg_sentiment\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the avg_sentiment and Close columns per day\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(aapl_df['date'], aapl_df['Close'], marker='o', linestyle='-', color='r', markersize=2, linewidth=1, label='close_price')\n",
    "plt.title(\"AAPL avg_sentiment and close_price\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation between the avg_sentiment_ma and the close_price\n",
    "correlation = aapl_df['avg_sentiment_ma'].corr(aapl_df['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "with open(\"Data/processed/amazon_comments.json\", \"r\") as f:\n",
    "    amzn_data = json.load(f)\n",
    "\n",
    "with open(\"Data/processed/apple_comments.json\", \"r\") as f:\n",
    "    aapl_data = json.load(f)\n",
    "\n",
    "with open(\"Data/processed/tesla_comments.json\", \"r\") as f:\n",
    "    tsla_data = json.load(f)\n",
    "\n",
    "with open(\"Data/processed/google_comments.json\", \"r\") as f:\n",
    "    google_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the dates\n",
    "utc_zone = timezone.utc\n",
    "amzn_dates = [datetime.fromtimestamp(int(d['date']), tz=utc_zone).date() for d in amzn_data]\n",
    "aapl_dates = [datetime.fromtimestamp(int(d['date']), tz=utc_zone).date() for d in aapl_data]\n",
    "tsla_dates = [datetime.fromtimestamp(int(d['date']), tz=utc_zone).date() for d in tsla_data]\n",
    "google_dates = [datetime.fromtimestamp(int(d['date']), tz=utc_zone).date() for d in google_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amzn start date:  2010-05-05\n",
      "amzn end date:  2022-12-31\n",
      "==================================================\n",
      "aapl start date:  2008-10-21\n",
      "aapl end date:  2022-12-31\n",
      "==================================================\n",
      "tsla start date:  2010-06-18\n",
      "tsla end date:  2022-12-31\n",
      "==================================================\n",
      "google start date:  2008-07-08\n",
      "google end date:  2022-12-31\n"
     ]
    }
   ],
   "source": [
    "# print the start and end date\n",
    "print(\"amzn start date: \", min(amzn_dates))\n",
    "print(\"amzn end date: \", max(amzn_dates))\n",
    "print(\"=\"*50)\n",
    "print(\"aapl start date: \", min(aapl_dates))\n",
    "print(\"aapl end date: \", max(aapl_dates))\n",
    "print(\"=\"*50)\n",
    "print(\"tsla start date: \", min(tsla_dates))\n",
    "print(\"tsla end date: \", max(tsla_dates))\n",
    "print(\"=\"*50)\n",
    "print(\"google start date: \", min(google_dates))\n",
    "print(\"google end date: \", max(google_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
